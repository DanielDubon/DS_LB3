{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLKrZBRklGHq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCJNGXd5l6sl",
        "outputId": "24bb9587-ea25-4531-bde1-2b9d22e5a5cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/mnist-multiple-dataset-comprehensive-analysis\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"agungpambudi/mnist-multiple-dataset-comprehensive-analysis\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XjTfhNI2moB_",
        "outputId": "6bb3135c-f288-457f-83cd-e5b3ffad0469"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Ruta relativa a la carpeta PolyMNIST\n",
        "data_path = '/kaggle/input/mnist-multiple-dataset-comprehensive-analysis'\n",
        "\n",
        "# Listar archivos en la carpeta\n",
        "image_files = sorted([f for f in os.listdir(data_path) if f.endswith('.png') or f.endswith('.jpg')])\n",
        "\n",
        "# Mostrar algunas imágenes\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, file_name in enumerate(image_files[:5]):\n",
        "    img_path = os.path.join(data_path, file_name)\n",
        "    img = Image.open(img_path)\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{file_name}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "XcPPbYm-nb_j",
        "outputId": "de786602-de15-4d8c-ce07-77f24d0597a5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABb4AAAExCAYAAACzsrRmAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUaNJREFUeJzt3XmYJUWV//8TmXlvbb0v9EKzyiKCLLKJIyOCyACjol9aVBRxWFzYdH4wDuPwFRSVVUQcARG6QRRxY1BwQ4dxGdTvAA2MIIKODY0svXdX13Lvzcz4/cFQjz0NnA90Vnd18n49j8+j5akTcSMjTkRGFUWIMUYDAAAAAAAAAKAmko3dAQAAAAAAAAAAqsTFNwAAAAAAAACgVrj4BgAAAAAAAADUChffAAAAAAAAAIBa4eIbAAAAAAAAAFArXHwDAAAAAAAAAGqFi28AAAAAAAAAQK1w8Q0AAAAAAAAAqBUuvgEAAAAAAAAAtcLFNwAAAAAAAACgVrj43oRcffXVttNOO1l3d7dtv/32dtlll41KO2VZ2gUXXGDbbLONdXd326677mo33HCD/P0rV660E0880aZPn259fX32+te/3u6+++5R6SuAseHyyy+3uXPn2pZbbmkhBDv22GNHtb31qYetVss++tGP2uzZs62np8f23Xdfu+2220axtwA2pkWLFtk555xj++yzj02ePNmmTZtmBxxwgP3kJz8ZtTapUQBUQ0NDdtxxx9kuu+xiEydOtHHjxtluu+1ml156qXU6nVFpkxoF4MX65S9/aSEECyHY0qVLK8/PfRSqxsX3JuLKK6+0448/3nbeeWe77LLLbL/99rNTTz3Vzj///Mrb+tjHPmYf/ehH7eCDD7bLLrvMttxyS3vXu95lX//6193vLcvSDj/8cPva175mJ598sl1wwQW2ePFiO+CAA+zhhx+uvK8Axobzzz/f/u3f/s123nlny7JsVNta33p47LHH2mc/+1k7+uij7dJLL7U0Te2www6zX/7yl6PabwAbx80332znn3++bbfddnbuuefaWWedZf39/XbwwQfbvHnzKm+PGgXghRgaGrL777/fDjvsMPvMZz5jF110ke222272kY98xN773vdW3h41CsCLVZalnXLKKdbX1zdqbXAfhcpFjHmDg4Nx6tSp8fDDD1/r60cffXTs6+uLy5cvr6ytxx57LDYajXjSSSeNfK0sy7j//vvHOXPmxDzPn/f7b7zxxmhm8Zvf/ObI1xYvXhwnTZoU3/nOd1bWTwBjy8KFC2NZljHGGPv6+uJ73/veUWlnfevhb37zm2hm8cILLxz52tDQUHzZy14W99tvv1HpM4CN67e//W1csmTJWl8bHh6OL3/5y+OcOXMqbYsaBaAqJ598cjSz+MQTT1SWkxoFYH1cfvnlcerUqfG0006LZrbO+Wp9cR+F0cBvfG8AZ599toUQ7KGHHrJ3v/vdNnHiRJs+fbqdddZZFmO0RYsW2Vve8habMGGCzZw50y6++OK1vv/222+3ZcuW2Yc+9KG1vn7SSSfZwMCA3XrrrZX19eabb7ZOp7NWWyEE++AHP2iPPfaY/epXv3re7//Wt75lM2bMsLe97W0jX5s+fbq9/e1vt5tvvtlardbzfv/WW29tf/u3f2s//vGPbffdd7fu7m57xSteYd/5znfWips/f76FEOw//uM/7O///u9H/jGWt771rbZkyZK1YsuytLPPPttmz55tvb299vrXv94eeOAB23rrrUf9zzEAm4L1rVFmZltttZWFEEa9r+tbD7/1rW9ZmqZ24oknjnytu7vbjjvuOPvVr35lixYtet7vP+CAA2yXXXaxu+66y17zmtdYT0+PbbPNNnbFFVesFffv//7vFkKwb3zjG/apT33K5syZY93d3XbQQQfZH/7wh3Xy/su//Ittu+221tPTY/vss4/94he/sAMOOMAOOOAAZ0SA+lvfGrXzzjvbtGnT1vpaV1eXHXbYYfbYY49Zf39/ZX2lRgEvPVWco57N1ltvbWZP/2P7VaFGAS89VdWo5cuX2z//8z/bJz7xCZs0adKo9JX7KIwGLr43oKOOOsrKsrTzzjvP9t13Xzv33HPtc5/7nB188MG2+eabj/xjuKeffrr9/Oc/H/m+BQsWmJnZXnvttVa+Pffc05IkGfn/q7BgwQLr6+uznXbaaa2v77PPPmv15fm+/1WvepUlydpTa5999rHBwUF76KGH3D48/PDDdtRRR9mhhx5qn/nMZyzLMps7d+6z/u24U045xe699177+Mc/bh/84Afte9/7np188slrxZx55pl2zjnn2F577WUXXnihbb/99nbIIYfYwMCA2xfgpeTF1qgNaX3r4YIFC2yHHXawCRMmrPX1Z2rcPffc4/ZhxYoVdthhh9mee+5pF1xwgc2ZM8c++MEP2jXXXLNO7HnnnWc33XSTnX766XbmmWfar3/9azv66KPXirn88svt5JNPtjlz5tgFF1xg+++/vx1xxBH22GOPuX0BXkqqrlFPPvmk9fb2Wm9vb2V9pEYBL13rW6Pa7bYtXbrUFi1aZDfddJNddNFFttVWW9l2221XWR+pUcBL1/rWqLPOOstmzpxp73//+0etj9xHYVRs1N83f4n4+Mc/Hs0snnjiiSNfy/M8zpkzJ4YQ4nnnnTfy9RUrVsSenp61/kzASSedFNM0fdbc06dPj+94xzsq6+vhhx8et91223W+PjAwEM0s/uM//uPzfn9fX1/8u7/7u3W+fuutt0Yziz/84Q+f9/u32mqraGbx29/+9sjXVq1aFWfNmhX32GOPka/Nmzcvmll8wxveMPLnFWKM8SMf+UhM0zSuXLkyxhjjk08+GbMsi0ccccRa7Zx99tnRzEbtzzEAm5L1rVH/22j+qZP1rYc777xzPPDAA9f5+v333x/NLF5xxRXP+/2ve93ropnFiy++eORrrVYr7r777nGzzTaL7XY7xhjj7bffHs0s7rTTTrHVao3EXnrppdHM4n/913+NfO/UqVPj3nvvHTudzkjc/Pnzo5nF173udc/bH+CloOoaFWOMDz/8cOzu7o7vec97Ku0rNQp46amqRt1www3RzEb+s9dee8X77ruv0r5So4CXnipq1L333hvTNI0/+tGP1spZ9Z864T4Ko4Hf+N6Ajj/++JH/nqap7bXXXhZjtOOOO27k65MmTbIdd9zR/vu//3vka0NDQ9ZsNp81Z3d3tw0NDVXWx6GhIevq6nrWdp75/0fz+83MZs+ebW9961tH/veECRPsmGOOsQULFtiTTz65VuyJJ5641p9X2H///a0oCnvkkUfMzOynP/2p5Xm+zj/Od8opp7j9AF5qXmyN2pDWtx5WUaOyLFvrNx2azaa9//3vt8WLF9tdd921Vuz73ve+tfq7//77m5mNjN+dd95py5YtsxNOOGGtfyno0UcfbZMnT3b7AryUVFWjBgcHbe7cudbT02PnnXdepX2kRgEvXetbo17/+tfbbbfdZt/85jftAx/4gDUajcp/I5AaBbx0rU+NOvXUU+3QQw+1N77xjaPaR+6jMBoyPwRV2XLLLdf63xMnTrTu7u51/u7kxIkTbdmyZSP/u6enx9rt9rPmHB4etp6enudss91u2/Lly9f62vTp0y1N02eN7+npeda/ezQ8PDzy/z+f9f1+M7Pttttunb8VvMMOO5iZ2cKFC23mzJkjX//fY/rMAWfFihVmZiMF53//I4JTpkzhMAT8Ly+2Rq2PoijW+TtoU6ZMec6XsvWph898//rWqNmzZ6/zbzL/yxr16le/euTrL7ZGZVk28rc9ATytihpVFIW94x3vsAceeMB+8IMf2OzZs5+3TWoUNQpQrW+NmjFjhs2YMcPMzI488kj79Kc/bQcffLA9/PDDa73//CVqFDUKUL3YGnXjjTfaHXfcYb/97W9fcJvcR3EfNRbwG98b0LMt7uda8DHGkf8+a9YsK4rCFi9evFZMu922ZcuWPe9L2x133GGzZs1a6z/P9y8dmTVrlj355JNrtW9m9sQTT5iZuS+Is2bNGol9Md//QinjB0DzYmvU+li0aNE6NeqOO+54zvj1qYfPfD81Ctg0VVGjTjjhBLvlllts/vz5duCBB7ptUqMAqKo+Rx155JG2Zs0au/nmm58zhhoFQPVia9QZZ5xhc+fOtWazaQsXLrSFCxeO/Et3Fy1aZI8//vhztsl9FMYCLr43AbvvvruZPf2Pcv2lO++808qyHPn/n81uu+1mt91221r/ea7fGHimrcHBQfvd73631td/85vfrNWX5/v+u+++28qyXOf7e3t7R35S9nz+8Ic/rFMonvmXELzQn9xvtdVWIzn/0rJly0Z+Cgdg45k5c+Y6NWq33XZ7zvj1qYfPfP9DDz1kq1evXuvrao0zM3v88cfX+UePq65ReZ7bwoULX1AuAM/vjDPOsHnz5tkll1xi73znO6XvoUZRo4CN5Zl/JH/VqlXPGUONokYBo23RokX2ta99zbbZZpuR/1x66aVmZvaqV73KDjvssOf8Xu6juI8aC7j43gQceOCBNmXKFLv88svX+vrll19uvb29dvjhhz/n906ePNne8IY3rPWfZ/6+0bN5y1veYo1Gw774xS+OfC3GaFdccYVtvvnm9prXvGbk60888YQ9+OCD1ul0Rr525JFH2lNPPWXf+c53Rr62dOlS++Y3v2lvetOb1vp7S3/84x/tj3/84zp9ePzxx+2mm24a+d+rV6+26667znbffffnLZLP5qCDDrIsy9YZuy984QsvKA+A0dHd3b1OjXq+f+zrhdTDpUuX2oMPPmiDg4MjXzvyyCOtKAr70pe+NPK1Vqtl8+bNs3333de22GKLka8/+uij9uCDD67ThzzP7corrxz53+1226688kqbPn267bnnni/o8++11142depUu+qqqyzP85Gvf/WrX+UwBFTowgsvtIsuusj+6Z/+yU477TT5+6hR1ChgtC1duvRZfzvwy1/+spk9vQ6fCzWKGgWMtptuummd/xx11FFmZnbdddfZJZdc8pzfy30U91FjAX/jexPQ09Njn/zkJ+2kk06yuXPn2iGHHGK/+MUv7Prrr7dPfepTNmXKlMramjNnjn34wx+2Cy+80Dqdju299972r//6r/aLX/zCvvrVr671j3KceeaZdu2119qf/vSnkZ98HXnkkfbqV7/a3ve+99kDDzxg06ZNsy9+8YtWFIWdc845a7V10EEHmZmt89P4HXbYwY477jj7z//8T5sxY4Zdc8019tRTT9m8efNe8OeZMWOGnXbaaXbxxRfbm9/8Zvubv/kbu/fee+0HP/iBTZs2bZ2/3QTgxfne975n9957r5mZdTodu+++++zcc881M7M3v/nNtuuuu1bSzguph1/4whfsnHPOsdtvv90OOOAAMzPbd999be7cuXbmmWfa4sWLbbvttrNrr73WFi5caFdfffVabR1zzDH2s5/9bJ2X0dmzZ9v5559vCxcutB122MFuvPFGu+eee+xLX/qSNRqNF/R5ms2mnX322XbKKafYgQceaG9/+9tt4cKFNn/+fHvZy15GjQIqcNNNN9k//MM/2Pbbb2877bSTXX/99Wv9/wcffPDI39VdX9QoAC/U9ddfb1dccYUdccQRtu2221p/f7/96Ec/sttuu83e9KY3SX+WSUWNAvBCHXHEEet87Z577jEzs0MPPXSdvxG+PriPwmjg4nsT8aEPfcgajYZdfPHF9t3vfte22GILu+SSS17Qby2pzjvvPJs8ebJdeeWVNn/+fNt+++3t+uuvt3e9613u96Zpat///vftjDPOsM9//vM2NDRke++9t82fP9923HFHqf3tt9/eLrvsMjvjjDPs97//vW2zzTZ244032iGHHPKiPs/5559vvb29dtVVV9lPfvIT22+//ezHP/6xvfa1r33enzYC0H3729+2a6+9duR/L1iwwBYsWGBmTx9gqrr4Nlv/enjdddfZWWedZV/5yldsxYoVtuuuu9ott9xif/3Xfy19/+TJk+3aa6+1U045xa666iqbMWOGfeELX7ATTjjhRX2ek08+2WKMdvHFF9vpp59uu+22m333u9+1U089lRoFVOCZH8o9/PDD9p73vGed///222+v7OLbjBoF4IV57Wtfa3fccYfdcMMN9tRTT1mWZbbjjjvaZz/7WTvllFMqb48aBWAs4z4KVQuRv7qOMWTrrbe2XXbZxW655ZZRbWflypU2efJkO/fcc+1jH/vYqLYFoD4OOOAAW7p06Yv6t5q/EGVZ2vTp0+1tb3ubXXXVVaPaFoD6oEYBGMuoUQDGMu6j6om/8Y3ae+ZfDPOXPve5z5mZjfxjewCwsQwPD6/zjwFfd911tnz5cmoUgI2OGgVgLKNGARjLuI/a+PhTJ6i9G2+80ebPn2+HHXaYjRs3zn75y1/aDTfcYG984xvtr/7qrzZ29wC8xP3617+2j3zkIzZ37lybOnWq3X333Xb11VfbLrvsYnPnzt3Y3QPwEkeNAjCWUaMAjGXcR218XHyj9nbddVfLsswuuOACW7169ci/YOCZf/EeAGxMW2+9tW2xxRb2+c9/3pYvX25TpkyxY445xs477zxrNpsbu3sAXuKoUQDGMmoUgLGM+6iNj7/xDQAAAAAAAACoFf7GNwAAAAAAAACgVrj4BgAAAAAAAADUChffAAAAAAAAAIBakf/llv9575LKGi2FPyuu/ulxJS5Y6ecJUnNyv6Rcpd9ole0JzVmaNvw8pT+eZmZ5WUhxEvUBOdS+K3EhastHeobR/xmUPkf9GGUumJlF4Wdjh712hpZslH3iw+9wY8oozslCqBlCjJlZHlt+LuGhqXM3qp9RkAjzpCEEJanWXpr6ubQYrcFU6HsIWg1W9pks+OspFWLMzCzx+5WlWi6lX4kwDomJhSX4Y5VEbb4rBe89n/i+lmuU3XfR2dUlE8ZQP0cJYy20p8xJM7Ng/vqM4oanfMQQqqsZilI4+5SF1l4RczdG+XxmZo2G32aV41AU1e1rVgrvDcocFWtiIu0N2riXQt93P+1sKddoe7BrnBvT1eiRcvVNmOrG9KvnqJ5eN+bxwl8rs/baU2qvuf3Wbszmr91XytWeMc1vb5vN3Zh0+mSpvU7w302+ZU+4MeXP7pXam/X//uTG9P/7b6Rc9//2127MksYKN2YgWyW119P21+ZW4r8XU9n+HhVy3actL3tSeAXtG+iTcm39mP/u/+1l/rhvCDOm+uupUrn/Dmem7Z1Z5j+0JNH2KOV8p54Blb1M6VerpY2V0p7yL6QdP3681N6UKVMqiTEz6+vz11Sj4a+nmRMmSO1VeX5V4pTnrJ59qqScFf/v5z7vxvAb3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1EomR4bghpRlKaUqikKIiVKuGP24RiP185Rae2b+OCh9UpV+cxajEGRmJnRL6bs8VGq/BEGYf0rfC2UQzKwUnnOqPBwzU+aMQp1W0pwR+x5DdXN5tGWJUqO0n/UpU7dMtXqXlUL9UdadPI+qq1FBGK4granq6nkpzN0Yc6m9QqgrIdGecxDWSikMaKYMupmZMN/Vn20rmYKQqyOOe1DOCtE/J5hVu9+Otkr7KhSpQmwvCnMuCf5xMe10tPZSYcYFbd0pHzEphPZyrb6WScONKcyv+Y2m1JzlHX8dJKnfnplZq+MPVlPI1WppzzkVchXivtbb8Me9Uwo1I2rzqhQmlno8KoVnOFa8vD3HDyq052/DT7ghm8XVWq5uP2SWsKYai9ZIza0YfNSP6QxKuTo77ODGbDFjsp8o117ZM3+p2PADN7sxswf7pfamDy1yY8a3/PE0M5swa4Ib8//WDLsxj242TWrvvpUr3JgHii4p16xyvBszach/hi97XJujU1Yvc2Pysi3lKtKWFDcWNIS9QKWcyfJcGxvlniJJqvt9U+XOTb2XU/qlxKhnXOVsoDxndS4o7al9r+ocn+fa+1KVc6Yqylx/IXEb0tgbTQAAAAAAAAAA1gMX3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1EqmBuZ5XlmjMUndmKTCK/my8PseY5RyKXFJog1rInzIkAQ3pigKqb0iL/2Y6MdEv0uyKCYrCr9fndIfh7L085hpz7mZdYm5/BilX2WhzdFceIbiMFhpWptjQWL+XApBKyylEBYKcSEI61yZb2lQa5TyGat7rmUQaobYXinM3VSod2o9D8F/hqU47iERakbw974y1RZnmjbcGH0fFWqGkkWowWZmFoU4oeabmcWyurPJaFPmpTInVWquRIkTYuS+C3FR/r0Mof4I4y7XKGG+lcJe1Gpp8zZKm7U27mXpf8aiI5yXhTO1mVkUCpD8lIVxiEpdEQVpztTPgC13Y/r6+rRkXd1uSDm8Wkq1XBjsYaFbRWxL7S0dXOXGpKuWSrnKJePdmOYff+/GDC3RVstQs+PGTBr2Y4ZW+2NgZvZkOuzGLGkOSrmWrvqzGzMk3Ed0L9FqwYw1/nxotbWzSCnsawOFf27LxTqW9ijvoNqcaceWFDcWNBr+GFZ5p5N1+3XMTDv/SO8c6gu6QLlnUuOUvjebTam9ri5/7o4bN86NUfeibuEZKvPKzCxNlTtMfzw3xnOWzsLi2tnQquoXv/ENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1AoX3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtZGpgYbG6VmOFuQRBut/X+hTC+vXlhSoLv1+x3AAdWYv68xJlTNVx9wc+SYR+Ra3vpTConU4u5YrCfC9jlRPL/4xJprWnrZ2xISTKWtEWS2J+XFTmm5lFYY6HUuh70NZKlAqCmquaWl2aNt+Urudlx41RxtNM+3wxUdemsM6b/gdsittykvvjoCqFqdwRPp+6vqzwa2eZF1KqWFQ3DqNNqRhV7gTy8hUalcqPuI8FZZqI604si64Yxfmm1E5hb4iFtlZSIVca1L1IOLMIazgVD8JJhe8N0fznE5SzljT5TFo8wpHjmVbVwI2uPbvtxvTNnqYlmzjODclXafV7xaoVbkynL3Vjhvu0tbIy859Zvsbvk5nZmoX+Z3y88HMNdvvPxswsb/jrfHVbaK8zJLU3e8YMN2Zgj1lSricGH3djZrX73Ji+P6+R2utb4T/npcPDUq6hpl8XB1N/LpSpP4/NzEJ3w40pxH10qKPNrbGgt7fXjSnFs6gS10y6pFzttj+GrVZLyqXo6vL71Wj4c8RMvD8RZJn2/qI8w/Hjx7sxfX1+LTAz6+npcWOU8VTjlHFviGOuPJtUrBlKnNKecienUu8Zqpqjm86tFgAAAAAAAAAAAi6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAAColazKZDFGMbIQYqq7k09CqCyX9hm19srSz6W0po57jEq//Jgo9UqlPme/zVTpeyil1pLE71fZ0XIpzycK41DK01gIjNq4h6S6tTPapHEW18qG/tRBqVEx1XJJz0x7/oWw7kLpr4NQ5lp7Qq6iEJ6zkMfMLEZ/L4q5OGdSf9yLlt+vsqnNvoYQFoI27lkpzAehdhaF+Jw7QlyunBPMyrwjxY0NyryssPoEca8WwqJQoxJxXymlz6it4RD8NhPlA4rDngpjGhJhnRdag1ni1331MSfKGan0110mnI/MzILQsVKs1aU0t/xciXp8ld9n6qWzvfC5Z4jn9ol9bkhr5XgpVWvpGjdmubK/dmnPNW/6n3HN0Aop11OrFrsx7aUPuTHJOK3v3ZMabkzXgJ8r22qy1N4Wr36FG9O7+xwp1yOrnnBjpizxry46jw5I7U0d9OtrV6GdRZ6M/vwb7vHHPReXVyGc9TtCn8zM1oj77Vgwbdo0N0beV4S42BmWcvX397sxrVbLjVHuH8zM+vr8+jp+vFZfs8xfU8pYNZtNqT2l7xMmTHBjent7pfYaDb8mKmNgpn3Gydts78YMiXeTyr1F958fkXIpc0u6jxDp98K+qvrFb3wDAAAAAAAAAGqFi28AAAAAAAAAQK1w8Q0AAAAAAAAAqBUuvgEAAAAAAAAAtcLFNwAAAAAAAACgVrj4BgAAAAAAAADUChffAAAAAAAAAIBa4eIbAAAAAAAAAFArmRoYY6ysUS1XUVmuNGlKuaqijpUSp8SUMVTW3rl/v5cb87GL79LaE7oVtK7bZz6yh5DLT/YPF94ttRdNeDaJ+JyFny+VZSlk0garVPoelfbMQplKcWOB8jhK0z53GfxkQVx34jTx2xMXSynEhaD9zDNErQ5XRap3wiMsCm3QlXUXxb0oEeZDlvlbbp7nUntl0OayIg9am26evC3FlcpnzLXPlxcdKW4sEMqKmXh+UOqB+psNhbBnaNOturNILLVcSh1+z6puP496GBHGIVHqT0NsTqhRat+/PKGatRJS7VwQC7/vZSHWV2EjlcZBrpvVnM83NQOz/fFZ3DMg5epu+OOzukvceyZ2uSHFyiE3pt0ZlpprdfrdmIHOoJQrS/z1EjstN6Yr0+Zbs99fB6+YspMbk2yzpdTey3f3cz28ZomU695Jn3NjfrNPnxuTfW5Hqb3WLP/ZbBemSLn6cv/5rBEeYUes5+3Sr+eDhXYmy8W9eyyYNm2aG6O9U2tx/SuWSrkGB/16IN1ZiftrT0+PGzNx4kQpV6PhH0gKYa9W8piZjRs3zo3Z542HuTFDQ37NN9P6rs6ZTkdYd8JcaAvvg2banJnU5e+PZtoZSYnZ0PecZmZJUs3vavMb3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1Eq2sTvwXGKM1SUr/VxBTKX0Su17WShRfs+C2N4n/7+93Jg09dv79OmvktqTCM/GzCwkwjMUHuJF/1hd35NEmzWnnnOnG1MqqcQlESxVgiRRGdSxQphLadR+1qesYXWdV1UzlDVgpv00s7RSyqUIyhwJwpw0syL6cUrftz3oYqm9NG24MUFcd/f+6O/cmEbhf74ias9GGdGizKVcibTP+P0qyo7UXtFpuzFlrvVdjRsLytIfw1Ssucq0VKu30qa0FwRt7h6zvNeNSaqsUULtTBKtRikjn0RlPMV6nvgVXYkxMztxtT/uyvlh3vQhqb08+odcdV9TSOdEsblSOptKh3gz8dwxFjR32NyNKQf9fdPMbGjIr/NJQ3sg0yaMc2N6ki43ZvlKrb3la1a7MUm/tvdMmTLZjekEf++M7WGpvdX9S92YJe0eN2baMr9emJnZo4+7Ie895CEp1SFv3cqNmRS73Zjefm3NJTP9ccjjgJSrDBPdmC6hZAzl4h1C8JMV4jt2V6bufxvfZpvv4sa0WtrabA37666MYr3L/bg/PbbMjZk5c4rUXpH0uTFbbfdKKdfQkL+nv/6Q/dyY/v5+qb12298b+pf/2Y1R+m1m1ukI9VV8p6/qTDbUru7s84h4m7t9OtUPqvBMJt1tiWf9tFHNlfWmcxoDAAAAAAAAAEDAxTcAAAAAAAAAoFa4+AYAAAAAAAAA1AoX3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALWSqYEhiW5MLMN6dWa0lGXpxoSQasn8YdBizCxGv1+f+Id9hDxag6UVbkwS/XFQ27Pgfz75Ry9ik1UJwZ/LaarNmcs/sbcbc8JZv/ETJdpgheDHJcLnezpQLhEbXRQ+kjyNKpxvQcklPI/StGcWo7DOk+pqdRAWcZAGwSwR5nip7DNK7TFtrEzcG3Y75Bo35v7b3u/G5MK+YGZWFn6cXKtLP64UxirP21Jzsej47eXCsxFzjRVBeB5q6VFquDaTNEGYI0et6JVyNYS+B7neVVOsS7FmzBu/2o05fsUEvz3hXGpmNm/ikBtz4ppxUq5Wx28zy/wa/O7HtJo4f3YutKflKoX5lygHSvXoI6weubyKc2ssmP7KPd2Y+ES/lGvo0T/7uYYHpVwTG/48GRr294xxTW0CTOj2z77FijVSrkbuf8ahxN/H8syvBWZmK1b78+3ulb91Y353xM1Se3+8/EE3Zqt3vEzK1Zk2zY2Z0T3RjZm4WDsXTJvpx6wZr717PWp+m0m76cZ0pdp7V5fwDtpRXozMLPFL9Zgxbpy/32WZ9vyz1I9bs0p7/lnmP7eqYszMGo2GG9Pd3S3lUu48hob8+jM4qNXzTscf9+HhYTem1WpJ7eV5dRNceU9VYnLxLFLVGdfMrNHtz5lY4XlFuhYW7pfNqhsHfuMbAAAAAAAAAFArXHwDAAAAAAAAAGqFi28AAAAAAAAAQK1w8Q0AAAAAAAAAqBUuvgEAAAAAAAAAtcLFNwAAAAAAAACgVrj4BgAAAAAAAADUChffAAAAAAAAAIBaydTAEIIflEQtWSnkEsXot1mWfp4QhCDTxkHp0/8EanFumkKM89vrlH6uxLSxsmo+niyt9Oc4QucLbRxKYW5dec7ebswHPrFAas+UOZpoa7C6lTr65HUnKJXnL7an9EvKpNRgMzPh2Ub1yQpthtTvfSi0sZq8xxluTCNtujFlrtXEkPoxMdFyJeYne8VBl7sxodTG6oGfnezGdIqOlEvaR4vcjSkKbazy3M8l11exzbqRnlmFNTEKJePrk1ZLuTpCt45fNVHKZeY//+smtN2YIJx9zMzKxD9nXD1j0E+kDIJpZ84v9mrj3j3Vr53ve6rhxqSp9upw4tIuN+bqGQNSLoX2mqLtfYWQrFReLsz0vXsMyDbfzY0pWoukXPnjy92YsqPtUSH4a7i1YokbMz4dJ7U3bsJUN2bVMu35t/JhN6YT/Zje8f7aNDMrhI/45Ol+jerOh6T2tjzoNW7MzFmTpFyNZS03ZmLun7XGrxFqsJltvsqPWSZuowPBD+wRzjVd4pGmJ/h7UafQ3otjKZzJxoienh43JgRtjwrCuT1JtFxZ5u+vSkyaauu80fD3176+8VKuri5/TFevXinEaGcR5R2g3fZrfkfcP+S9WpCmypzx111bWL+q/WbsKMVF6a7T/3zqNa7wii32SXxvFPAb3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1AoX3wAAAAAAAACAWsnUwCT6MYWaLJR+TAxaquDHxSh0vkJqe5/4x1e7MSFU13elX1rfhecn5jr3C3+oLJcSU4jP5rzTd3Fj2u22lCsIiycN/s+glLluZlYKn7EsxXm1YZfOeolSzdDmrtSe9jikIVSerfJcn06m/DxT63x763f7mULqxiSJ9jPWopP7QcLcjVHbjcqyur5H83MpzzkV8piZvfyAL7gx9//0g1Kusuj4Mbk/pnkuPD8zK0o/V1mKa7Wobk2PtpB2uTGpWHSjcOJKEnGvFs5kQaidea7VlVRYUx2x3n1l0qAbE4VpqW51IQrrvC3swQ2xQUEj1WpG2fGf4dVT/PF871J/HpuZFYU/8Mc+ruWaN73fjWnn/ryKpbhxC2dvaas1s0Q9b40ByexXuTFhqEdL9sTjbkijs1xKtWLJQjcm9vl51gyukdrrtqYbk3eGpVyF8nbc8Ndme7X2znHdJ/359qqn/PU0fc4Uqb3BHr+9P7X8M4aZ2ZbT/HrwwNH+Gt7/Mu0ssuoJ/xmO6+mWch3R8mvnD3qmujHdqbY5FMGfV4O5VnvUd+OxQHnnsKA9/5AKe4a4ZSSZ8D4hxJTiaUTpe3evVquV+5PBNSvcmIF+rb4q53vlfaIotHc95fOp73rSO6iQqxDPba/b4pV+e8q9qpnFWM3vO8dKL4e0cYgV3d/wG98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANRKpgb2ZsIdeYxasujniomWKwpttns27P3+h4/fU4pLgjhejhC0uMRSIap0I869/EGpvRD89koTx6D0P2Qs/Vyx9D+fmdkZ59/rxlz2D6+QciXBn3+JMN/b6vKKwmcUc1U1RzeEUni2QVwsyhgqtcfMLDehTWHuJspzfTrSb27792qZlPUi9N3EvhfCfItCjVJizMzK6NeoVFi/ZmZp6m+nyvyL4ppLgt/ebodcKeW687vHuTF54Y9pkYvjLswrJcZMLmVjQoyFG6MsJzMzU+aJOHeDkCsI8y3JtPr6ziV9fq5EyyXNE6H+BOl8ZGbmP0Ol5CeJdvxW9pkorE0zba1Eoe9XTx2Q2jt+qf+c1X00SYRzlFDPSyGPmZkFYUyDMBfMLMiLegyYtoMbEoTzuJlZ3+Djbsxgc7GUa2XnT27M5MlNN2Zc3i211x2muDETh3Ip11NLBt2YTKg/n/n0cqm9/pX+mN55z0NuzOqHtbUyfrft3Ji5p82Qcg0KQ7q8R4iZptXz4U7bjelZ4z8/M7M8+J0vuxtSLkWa+3WlS3gPNzNrdW1CvwcpnA2U+wcz7d1bPYuq75eeotD2FWXvXNW/SMqlfMZm06+vnU6nsvaUGPX8oNjQuYJwF2pm9vNH73djXrvly6VcSr8K4aSojpUSpd4DVrW+NqFKBwAAAAAAAACAj4tvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1AoX3wAAAAAAAACAWuHiGwAAAAAAAABQK1mVyUIIUly0UgjSciltBotSLsWpf7d7ZblK8/ueBL/vUfx45115nxvT6hRuTFH4MWZm0nRI1Ofs/4wmTdNKYszMojConbY2Dkni5wqpH3PNmTtL7Z14wQN+e+Ja3ZQoz0yJ+Z9IN6IshTpmZqVSf0r/ecSgzd10p2PdmFBofZcEP1cpfD4zs/DYdW5MLjzDtvhslFo24RWnSLnK0u/Xk3ef58Zs+er/K7WXCD+3DsL+YWYWhf22FMpdKe7byt6n9OnpuE3n5/dK/VHmUZXtmZklJtQWISRJqttf1b6nyjRJlbUiNSfFSZ9Pa05S5X5e5clAWsLiQBz91Hg35qubDboxQTiPyeRxr/Jpj7LeSX7MFltKqXqKl7kxQ+G/pVz9Az1uzIrFK9yYzXqbUnu9eceN6fQ2pFyrhnI35vJvDbgxA6v9+W1m9siKP7sxd/3m536iYa29+8tfuzE/nyqlsvFtf01NDf5cePJQ7dk88sMn3Zhte3ulXH1COegINSOJ2rul5X5cLLTaM5T6832syFL/2abaMrdU2aTEd69E6FdI/Gs39fXsU58+3Y256aabpFydjv/8g3B3VxbaPJLOd0JMleeVoGaTltSGvWMRX7GlrkfhHaQUz+fSO484VjGp5l1v03ljBAAAAAAAAABAwMU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1kqmBSeLfkccYpVxBiUmlVFKbafSTfeDYXaX2gvARQ1A+oVmMpR8j5Lrky/dJ7Sl6uhpuTCy1z1eYP1ilPwRPtxlzIUiYo1rXJR+96jEp7tzj57gxiTCvhCVoZmaNtLoPqc7lsaDV6bgx+tr0H0i0QsslrBdlXqp97xLCSnEyxSh8xoU3uCHq3LVE3pKc9rS9KEn8ujLw+y9KufLSz5Vl/ud79NeflNrber9z3Bh1T/6rufPdmJ/d8G4hk9aeUqt1YptjgPQ8xGem5CqFM8bTuYTnIaQ6dvU4qb00E/ouZdLOpkpMUA53ZlYKh5ay9Otm0dHquVL3lc+n5lKU4t539dQBN+b9KyaIrfqfsSj8fsVSe7kIiTADg3AuNbMgrukxoTMoxKyWUg3HITemP9PGcHXmP4/V/hHQYhQ+n5l1rfHfhQbaWpX61k/9599ZscKNyVPhA5rZkK1xY+IJi/1EE5pSe79a4D/D1X1SKmu3/LGaPth2YyYs1ebVn7eZ6sYMi/cR/9432Y2JqbBHVvjapexXZmb9eau6RkdZTIQ9MWjvEsrWmabaBGg0/Jqh5Lr11mul9p566ik3piO8F5uZtVr+81fOGep8q4r6jlNlrsraDNWNlXoGlJ6PcE5US1QpRKr3clWdX/mNbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaiVTA0MIlcSYmVmMQogfo0qS0o0pi8qaM3UYQiKMqd91S9UfX0Q/MMv8KVGIzyYU/qAWpZZLCSuFwRIfjTSXo/JwzCxaKsT4z0aJMTMLVuFkrm4ZjrpOkbsxSaKNoVJ/YiE+f2HSlULMjP1OkdorlXUnl2p/HJR6UCof0LTnE4LfnrwXSVugP6/MzNLg51JKZ5KoNdHvV6lMPjNTpvLrjv6KG3P7V94jtaf8zD1Gtb5uOkVKebRVfppE2PPNtHUehDUcUn+v+58G3ZDre1ZIqUptmrjErUGq1VE425UdreNKLRNLhnQ4lT6fUIPNxLUpjJWZVg/et2q8G3PNhDVSe4XQXipuM6VYy8aGVX5Ia6WUKQ76ubraHSnX+KTbjembOOS3t6ZLau+YAx92Y3pmSalshvnPvx1absy/HP9Kqb3ZM6e5MYO7b+XGPB798TQzWzq01I3pmjRRytUZXu3GFNZwY37wO23/mCHUstWpVu8mFH6/2sJbaKLWV2VvaGjXPMq7/1jRyf31lIt3C0qdD6n/XM3MEmEIk6zpB5XaO4cSFwutvpZ5W8jlf8AQq7t/eOUeh/rtie969951qxuj3jtWdUYvxINbItSMXz50v5Trr3bY2Y1RhlQ/0lTzbqFl0vAb3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1EqmBiZpcGNijFKuWPq5QvBj/qdRN+S9b9+1uvaC316qDYNZ6Qd++Wv3uTGxFNsLfmBetIX2tLGKwudL1GFP/Z/RNBI/JhFizLT50Cq1gc/Nz5UGv19RiDEzy0ydgD55XYwBnU7HjYnihFNqWSIOcy48j61ff4Yboz6J0lI/V/DHysys/775bkyW+u0lwvo1MyuF+pqI60ChzO/gfzwzM0uEepAK+6iyL5iZFbFwY3JxbxC2BmXrs0bWlNorcn/+leLP5UuxDtdNpbVZSKW0p5wf1FyqUlgvUTgkxURb6IkQJx0zxPNykD6fePYW4qRnI5bghrA3KPu2mVkp1M4k+DUxCPujmZlyjFLH3Srcs0bd0CN+zLJFUqr4p4VuTPb4UinX1OXCOljhv9IOL9FqVD7L38vCDCmVDTVbbsxn3vRKN2blwGqpvfbgkBvzyKN/cGP+tMZfT2ZmT/b5MUPjpVTWFrbzMsvdmJkD2h4zLfpxQ8ODUq5JseHGrGn5Yzpg/vMzMxsUilTR0J5hT+r3faxQjnzqGSMVzqyNhnauXbx4iRvT1dUttKddzU2ePMmNabWGpVydjl8Xh4Rp2W5Xdwbs7u5xY9Tzf1H4NaPK+yElJhHf6qtqz8xMKHfa2Ue8S1HeG9VcZVHNu94mdBoDAAAAAAAAAMDHxTcAAAAAAAAAoFa4+AYAAAAAAAAA1AoX3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALWSqYEhhEpizMxiiH5M9GPMzIL5bZalkEfrupnQnvL5zMyu/8a9fpCQKoo/viiVz1gIMan4nIV+lVHMJQ2p8KDFeaWENcQ5k1vqxhTCw0mEuWdm1vCbk+nrYuMb6rTdGLVGScS5pKy77q4uN6YolMVpVib+OgiFVvrzPJfiPOqUTIISKXy+Sp+zVmCVvUjpVZKoz0aZ7+rI+2Oq7MmNTOt7IjyfUpvu8roYC5QxDOqGHoRnpmXSzncV/pqENA5i1YjRHwdphhTaaClzVxnPVDmvmJl4RKqMNhe0ToXEnzS58PzMzBJhNitlP4oDmii/FySsQbMXsKbHgPy+n7oxnUVPSbmW3nunG5Msf1zK1bVqpRsz9IQ/R952lbbOi6TlB22mzaVBG3JjWisG3Zh23pHaa3f8uHHCMEwQj38t/+PZhB4t17hXbuvGZJvNcGMeXfBHqb3hgcluTGPb8VKu8aW/ZyXCbtQybeBzoSYWpTbfs456WhgDhP0nShce2p6RZNpZJKR+nVe2H/EKyUx5tuLzj4VwnhRi5L4LZ0BlrNTzkdIvte9Kk1KM+m4pdEw/JvrPUFo64lhF4RylnaLMyop+V3vTOY0BAAAAAAAAACDg4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANRKpgaGECtsNlSYy1dGoT3x44Xg5/rmt++RcsXoN1oKXQ9i5xNlHDLlZyHaz0sK5fOVUiorhUAlV1kUUnvKs/nw+3eRcilzJhFiSvE5Z8L6UvpkZpYkm87Pxtp5x41RP7dCLYnKFM+EcU4Sre9F4bf4h5+cJ+XKmg0/qBTmiDqPpILg51IfszYfxAct1AyltRBSrbkyd2NKZQMxszwXcgm9b2RdUntp4j/nItFqtTpeY4M/d5W9zszMghCXqOc25YwkPP9mU2otCp9R2YPVOCWTulaKQljnQqq84+9XT+eqcD+vaP+Tn43wnNNUW7+p8LYShLpSllpdUXbuIK6vCo8do2548SN+0FNLpVzJ4iVuzKRBbR2MK3rcmM2vGXBjuidLzVn/miE/V0M4H5nZvDkvd2OWtYX3hFQ7R0VhbwjDfp7NJ0jNWRj0Y1rKOdHMLtrx825M78Kn3JhXPbpcai/0+GPV3LJXytUb/CKVRr/eDYlXM2uEM1Ir0+pdWbSluLEgmLBniMfCKOznaUM714bUrwdROK8m0lujWYj+s42FVl+VuFgIG5m4vypnFmWsori5KmOl3nYE5Y6lwnuYIPQsFd9TCyFOOd6V1b0WSzFmZibegbhpKskCAAAAAAAAAMAYwcU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1kqmBIYQKm40VxZjF6MfFWGXfhVyh1DIJqRItlUQZ0XZ72A8KqdRekvg/V0nFXGnqD1YUBjQm2lwoS3/gYyqOgzRnKooxsyA8aXVFKLnGilZR+EFB+zzKMwullisEfx00mv5cUuakmdb3vBTGysws9/uufD5ViP6WpJQMdZ0r+1owbdyVvUhpr3vHY6T2lPnQ6XSkXCH1n+GyX33WjWk0GlJ7ylQOIZdymalxG5+wJZq6nJRzTYxaspj7c7cU2kvTptReHv1nFhKxVktL3R+HKP4eSEcZU6HrmXxOFGpwop1FLPXrayz9AS1ycf8o/eecdGl9D1E4kxU9bkyaDkjtWdnlxwSt3hWhpbU5Boxb8DM3pnhYW5vlMn++dSdbSbmse4ob8njxmBuTTZyjtTd1khvyRKbVjEcLf7z6e/z5tqLPHwMzs8eGh9yYh6b462CVn8bMzBKhW/u+Zg8p15++/2M3ZtkDD7oxqw46UGrv/+z4Tjfm4T2mSbl6Jvgx6Ro/ZijXauJA6defZS1tT17W7JbixgLlfB8K7R0gEXJ1dQl7gZllmXylVgnlnSPPtfOx8q6g3OkofTLT3oX+40dXuzGpeA+jxFXZd0Wq3sRI3RLP+sqdqXKXJt5/KCfFKN4DmnA2VfAb3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFa4+AYAAAAAAAAA1Eqmh5aVNRqCElPdnXynEPoeopQrSfx+vfWIPaVcN990lxtTBL/vZSE1J+nqaroxMWpjpYTFUptXpRCn9CtGYfKZ2ftPfJUbkyZarlzpe6mNqaKRpJXlSipch6NtuNN2Y5TaY2aWCPNELBn2lhO+5MaU5i/iItUaDEKtbnVyKVdp4oA5YinmiUK9M38c0lKbt4mwhpOXHSXlCsFfd0GYNKVYX0Pw50wUN4fh317nxnRlDTcmE+tFUfifMQva8aQVWlLcWBCVdZ5rz0zZ74JY8NLM3/cbqT+/22v8Gmym7eehW+u7Mg5Ke8LRzszMMmGda4dcrUHpXCOeo0w4Tyq1WpnHZibtHvp50o+b17vET6S+ykjjrqVS59ZYMLS6341ZOaidH1au9udJ0hmQcr3uSz9wY6Zl3W5M1tYe2u96u9yY1Y0eKddA6dfFPPH3uyzTJtL4CX1uzNQJfq6uviGpvW2/7c+Zh+55QMq15cDtboyynsZPGS+1t2z1UjemS7wpaTb9M5IJ+1o6pDXYiH5cl7Bvm2nnu7FCOmurr8HCu16zS1vnyjlK6dh+bzxeau+H37jAjSly7UymxBWp8F4snjmDsoilSySpOSlOfdtNhM9Y7T2n33mlT2ba+7My7Oq1o/YIxYdY0UFqEzqOAQAAAAAAAADg4+IbAAAAAAAAAFArXHwDAAAAAAAAAGqFi28AAAAAAAAAQK1w8Q0AAAAAAAAAqBUuvgEAAAAAAAAAtcLFNwAAAAAAAACgVrj4BgAAAAAAAADUSraxO/DcysoyJUl19/vBghAVtVzBj0uj316aSs2J/D6VyhCYWRT6HoUxMDMLiZ/r3e/aw88TxM4LYqb1vSz9uBj9mGuv/b3UXlFWt3aUOTpW5LnwuRNxvglhqWkL75tfOs6NOeoD89yYRKwracMfh9cddaGUK89zN+bO7/5fKVdVNtvrRDemo8wF09adEGJmZqUQWGipNFHI9tA3pFSNRJnL/j6aiGsiEfZ3tfTEtKkFbiLSVDuvlFKd1/a7MvrrXJm81/eulNp71+pxbszR/eOlXNcJbSq1Uz0lhtjxg4S5q+6sIfhrSs2lzBmtJor7aIXn+EL+lM9P7XsUxko/T24656iie4Ibk04T6oWZNcq2G5MPaK+h4wf95zFzZp8bc/4sP8bMbGGzx43Jcm2/WyrUjNVtP6YljKeZWWj6Y9q44RE3Zk5Xt9ReJuzB2++yvZRr+tQpbkx7cMiNmTjZn8dmZg+ftp0bkwV/LjwdKNTqhr/TxFLcjQrhfkDYP8zMmhXW6tEWhDNSIo6hck/R1dUl5UqrvYxxKfu58g5nZtbp+PVH+XxZptVzpe/K/qru52OR+o4Tgj+XK7zaskIaU63BKMTJ50lhHBT8xjcAAAAAAAAAoFa4+AYAAAAAAAAA1AoX3wAAAAAAAACAWuHiGwAAAAAAAABQK1x8AwAAAAAAAABqhYtvAAAAAAAAAECtcPENAAAAAAAAAKgVLr4BAAAAAAAAALXCxTcAAAAAAAAAoFYyOTKUo9iNZxGru5O/5ZY73Zg3vWnvytpTvfmIvSrJ872bF1SSRxVikOLe8rZXVtdmENoU5qiUR9SwXIorS79fX//6Qjcmxii1Z8JnVHNVOV6jLS8KNyYKz8LMLBE+d2laLmW5NFO/FBemPbPC/HFIMu25JubH7fXmT7gxea6tlVanU0mumPljYGZWRP8ZKuvXzCwRHk/nv653Y9Kg7X1F4j+bLNFyJZb6QUKICXPPzEwpP6mwJtRcY0VZ+uOTJdJAW5r6cbnQnplZWfhzvCPUH2UNmJllDX9eil23Y9tT3ZhrG8v8ROI6V/YQZX9N0qbUXrWUz6g8RO1BH1dMdmNC0HJFoVYrxObEYVCTiZN5DBiODTemlWn7+VAx7MasXt2Scm01faYbMyH643zJw09I7R2z/Q5uzBXL75dyNZs9bkxvxx/37lXaPOoZ8OflpBUDbkyrUM8Pfntpt5ZrypZThPZ8v3/3tlJ7E5t+jepfMyjl6pR+zzrC0XtYvP5ol/64K+8DZmbJplOixPsA9d3Vj2s0/LVppp3JqqScMwrhvViNU2IS9Z1DiFM+n3qXId+fbEDq/YoSt8t2W0i5tDFV8kjNSe0JZczMxFdQAb/xDQAAAAAAAACoFS6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArWRqYAhhNPvxLA3GClP5fb/le3dJuQ5/0x5+UHVdl/ztm3evLFeocNzNqpszsbJuVff5vvGNh6S4JFF+vlT6IeIgBCuqy7Wh1/16yEv/c5dBGGczS4R5kog/N1SW1NWXHOXGnHj6N6T2yuD3qyiEOVIhdR6VQs2ISq5Ca2/Jf3zRjdHWr1Y7MyFXodQCM7Po5ypLdb5Xs87V56zEpWqfhPk+VhSF/zxihbVZ3TeVNRWE56HUYDOzq9MVbsxxNlnKZULfj82nuTHXZkvE5qpZK+pzVuuPoqrd/ITWRC1Q6Lr6+eZ1+XMrJso81ijvDSpl7YwVy5f745zlWq7e0n+277xGqxnj05YfY37HuttDUns3PXK3GzM5dqRcA4NtN6Y93HRjeoe7pfaa7YYbM358nxvT0+XnMTNrZn7cXXukUq5O7tfF1rCfRy3TzYY/plmzV8pVlP5nHBbK3VCirYlhYUiLXFusSbmBLy5Gmb5PC2fRVFsHSaJcqSn7nbYnHnvq59yYf/qnc6Vc1152mhtTlv5cisJ7ydNx/nzTjkjqc1behcR3L6nN6vb8V+64hRsTxa6XFXVLPh5VWFaqOntvOm+MAAAAAAAAAAAIuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWMj00VthsqDCXL038jxmj9vl+cOu9bszfHLarlCsEfxyUmGpt6PY02vNJ3Yhbv+c/P1W70OZMCFWtnerai7EU2xyb8+HZ5MJniqU4htH/3KX4PGLw+6Ws88vP/z9Se1nw612SaD/zbKR+riyrJsbMrNlsujFdWcNvL/VrgZlZKsSlpTZWWfDjSqFbibjkMmEJJ2IyZf6VQipxGzXlZ+5R/rH8plOjlHFOhTWn5spLrc5HYS+Lwr6SZf76NdP2qGvCSinXCXGqG1MK9fw9bT+PmVlRFFKcRyhjZmb2leaAGxPEeh7Mnw/vG5jg59ng51KTCmMQ6op8HkuFMRXPE1W+PY22VYvbbsw0sTZPG/Rr2cLmkJRrXNpyYyaO63Fjeoe1p9EjnA0ycX/tH1jjxnTW+OOe5dre0NXxH9Cq1nI3ZvCYvaX2hlsdN6a3a6KUq1X441AU/lxoD2l1ut/89sZPmyTlGkr8uTWoxDS1BdbO/c+onslCWc2+tiG0hWdm4juH8g4wIe2XcnV3r3Bjoq10Y4ba/vw2M2sVU/wgcd2d8snvuzFXnLmXG5M3tXevZpdfqxs9/lkkS8ZJ7QXhvbiM4qHMutyInXb7azcmFe+QSuEcLx71rVDuSYSiEdXfmxa2yJBocyaI70YefuMbAAAAAAAAAFArXHwDAAAAAAAAAGqFi28AAAAAAAAAQK1w8Q0AAAAAAAAAqBUuvgEAAAAAAAAAtcLFNwAAAAAAAACgVrj4BgAAAAAAAADUChffAAAAAAAAAIBaydTAEMJo9mNUBduwff/h9++T4v7msF0raa/KZxNjrCxXlW793oKKMpUV5TGLqdhi8MdUeYZlqfW9jIUbE4U+mZmJYWNC3hHGJ6jPX1lT2s8NtajqBroMuRuTRLFXQj1Q56UiFca9UwpjlYpbW6asTW2s8kSIU8ZTrOcx9QtQJtbzROm7QN2JEqFb6r6diM9nLEiThhAjH8tcMXakOGVJKeOcCnPSTKwrifb8v1QsdWNOsKl+ImHfNDMLFZ0hEvE5v7vV6weV4loR6n6QdiytrsxrrnBjSm3YrYjdbkyiHFjUmqjEqQVvEzpHLX900I1J2y0pV99i/yzy5CvGSbn6hZhFTX9NPTVlitTek0N+34M2DDZz4iQ3ptPnT6aFYaXU3vJev+9DK/2F10q0et4WNpDWsN8nM7Oi7Y9Dd+LXxO7eptReLtQ7YSqYmdma1O97vzCkg+K7ZRH9vShNteLTyKs7x29KqrzzCOY/OOVdX+1Tle9eSr/e+/FfuTHtdltqrz085Mb87GvHSbkUu73uBDcmFv7e93Sc/xmLfNjPIz7nGIX7oQrvtky5H5CvHat7PxOGQbLpvDECAAAAAAAAACDg4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANRKtrE7sCGUpRIVKmxRy/XD799XSWuHHPrKSvKo5H6HOLod2di0iWUhKD9f8scqiOOZCM2VpfhskirXxegqhecRo/a5S+FjhyA+f6HNRKgZat9DUJ6Z1vcy9WOiMN9SK6T2OoXf90RoMBZtqT2pW5m2TSbCGi4TP6aRCIP+dKAbotQCM7NY+PMhFaZVrLBcJNI8NktScbzGAGXL6HQ6Yi4/WUes86kwxxvdPX6f2i2pPaVGaXXMLKZ+36+yVX57DbE9oQ6f2B7nJ1LXplLv5O3cz3VV91I3Rn02SkFQcxXRn++lsK8p+7GZtr7k3xzahM7Cwyv959GIfVKuabkwht3a2WCcUDvXpP6+PzxOa6+/4dfhzqCWK+3x49pCDc6VA5mZdRI/V1eXsFYaTak9E84Pa1prpFSDQ8NuTFr6/WqK5/NcOG8Na1uyDQjnu9VCvwZSbV5FoSY2g9b5nlJrcyyQ3oXUTbGq9swsFc6iSozaXlH4z0x/b1TiqnvHVuL2OeJzbkzRHtDaK/y4KL7AaHcNfq5CHSvh3VIed+Fyo1Tuo0zbi7Tr1w17z8RvfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaiXb2B3YEMqyrCxXCKGSmKfjUiHK7/ttP7pfak8RY/SDghDzdOB69eUvSf2qkNReovUpBv8ZSvOq1NoLwvNJxB95iVN5TCiE4VGnURKrqxlae8IDETsfpPmm5cqF+pOUDTemU4oTrlHNQ2ym2taWpx2/uUKbC4mwqBqJX/PzVHs2qTAOSablyoR+ReHzBVPnqFDvxP0jqXCfGW1JWrgxIYqfRzg+ZIl4xBPOIkXur4M4XmuvFOpB9JemmZmFPHdj0qLl54n+szEzKxt+36/s7ndjkrZy/jPpOZtQ83V+PVfOpWZmMRHWeSauc+HxhND0g8RhD0KDSdD2tSz0aY2OAa0h/zMtXdOWcv150F93m4m/f9XV0+XGTNl6shuzZLI2d5tCvStX+rXHzGwgX+XGxMzve1OcR8ODQ27MlEa3G1NmWj0fbg24MQNCnTYz62TCWWTQ3xzWmLaBNK3Xjxk3UcqVB39dDCX+OHS6lBps1j3e73t3S9vXknK1FLepUO8MlDj1vTHL/P0nFd5N1DurvBDeX0x7/lWJ4jkqCmeIMvprpSy1uhKVMZXnjH9mUZpLSu3so8yGUnxviMI7Wqnsyep9RCocuJT7DxP7JeA3vgEAAAAAAAAAtcLFNwAAAAAAAACgVrj4BgAAAAAAAADUChffAAAAAAAAAIBa4eIbAAAAAAAAAFArXHwDAAAAAAAAAGqFi28AAAAAAAAAQK1w8Q0AAAAAAAAAqJWs2nShskxlZZnMGmnFH9MRYxTjlCh/TMtSa0+RKD8KidU951jpkxbaE5+NohkaleWKwjNU+x6UdSg+wirHa7QVwryMUZxvwc+VlIWUqgjCoiqFfonPIjE/rjSt76lQO6OQK09yqb0Y/VzSnCy05xxTPy5NUymXEhdSv+9qdU2EYYjK3DOzUmg1CGtCCDEzs0TYaJSYFxI3FhTCvEzEGRCF31uIQi14JtLn970ZtbNWHv2+t6NWM5QxtcL/fKk4j7Lcfz557texQuiTmbbu5CWQVHd2U1R7flBquvD5SnFeKc9QymRWBq3NsWCNML+TXNsTu4WYtjiKMRl0Y7Itprgx+VTxqbU6fkg5IKVa82TbjcmHW27MQHOi1F6WNN2YVUuXujFFb5fU3oDwDIumNmeyHn/WNPr8gpd1tKLY2zXOjVncGZJyFcEfh7bw3tCf+/PFzGyg9OdMx5/GZmY2Xj28jQHKvqLfwyi5tLFR3gGyzD8jqXc6ee7vK+o4KGcI5f25FN+LS+GdV+m7ksfMLCpx4vWA8nyUvotHQOnVX7n/UHNJxLOk8p4SxFxlRXePm84bIwAAAAAAAAAAAi6+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAAColWxjd+C5qDfy5aj2YrRt6N4r7VX4s5AQ/RghZKwKIUhxMfofMiTKWImDJcQpffqfSDFu4yuFvkbx84RSeGal9vwt9UNyoVup2F4UwpKorfNSmCelkCq3QmtPivKFRMwkPOey1HIpccoyV2LMzJKG/6DVvieJMh/8XCEIk9202plJfTILqdbmmJA0/RCxrGi0ZEEZa2Gc8+G21F7W1efGJN3+WJmZtWzQDxpW6o+2VlKhdmbCOhgqOlJ7linzW1wrFc2tQtw/olD3o1qjlP0vCM9ZqPlmZjH6uYJ4jiqUQ8AYUTb8ddffpe3nf277cf1d2vPvTPXjmuNzN2b5ZK29wbY/x9csl1LZoDBNyoEhNyaYXzfNzCb1jXdj+lv+OMSG1JyFpj+/Y7e2BsrMX+ctoVbHQpxXpT/ulov7WtN/0KlwwFPfz9q5v4cMidtMd1Wbwwagv79WkyuK715Z5tfOLPVj1HN7nvv1TtnHzMQ7iCCsO/Vdr/T7rsSocyEKL8bqWURpsyyqez+T2qvwriYI58movltIZ1Ox9oRq7if5jW8AAAAAAAAAQK1w8Q0AAAAAAAAAqBUuvgEAAAAAAAAAtcLFNwAAAAAAAACgVrj4BgAAAAAAAADUChffAAAAAAAAAIBa4eIbAAAAAAAAAFArXHwDAAAAAAAAAGolxBjjxu4EAAAAAAAAAABV4Te+AQAAAAAAAAC1wsU3AAAAAAAAAKBWuPgGAAAAAAAAANQKF98AAAAAAAAAgFrh4hsAAAAAAAAAUCtcfAMAAAAAAAAAaoWLbwAAAAAAAABArXDxDQAAAAAAAACoFS6+AQAAAAAAAAC18v8D2w4cl0izIWEAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "base_path = '/kaggle/input/mnist-multiple-dataset-comprehensive-analysis/PolyMNIST/MMNIST/train'\n",
        "modalities = ['m0', 'm1', 'm2', 'm3', 'm4']\n",
        "\n",
        "plt.figure(figsize=(15, 3))\n",
        "for i, mod in enumerate(modalities):\n",
        "    mod_path = os.path.join(base_path, mod)\n",
        "    file = sorted(os.listdir(mod_path))[0]  # Primer archivo\n",
        "    img = Image.open(os.path.join(mod_path, file))\n",
        "\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f'{mod} - {file}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2f430a58"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def load_and_preprocess_polymnist_single_cell(base_path, modalities, img_size=(28, 28)):\n",
        "    train_images = []\n",
        "    train_labels = []\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "\n",
        "    print(f\"Base path for loading: {base_path}\")\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"Base path not found: {base_path}\")\n",
        "        return tf.constant(train_images, dtype=tf.float32), tf.constant(train_labels, dtype=tf.int32), tf.constant(test_images, dtype=tf.float32), tf.constant(test_labels, dtype=tf.int32)\n",
        "\n",
        "    for mod in modalities:\n",
        "        print(f\"Loading modality: {mod}\")\n",
        "        mod_train_path = os.path.join(base_path, 'train', mod)\n",
        "        mod_test_path = os.path.join(base_path, 'test', mod)\n",
        "\n",
        "        if os.path.exists(mod_train_path):\n",
        "            for img_file in sorted(os.listdir(mod_train_path)):\n",
        "                if img_file.endswith(('.png', '.jpg')):\n",
        "                    try:\n",
        "                        img = Image.open(os.path.join(mod_train_path, img_file)).convert('L').resize(img_size)\n",
        "                        train_images.append(np.array(img) / 255.0)\n",
        "                        label = int(img_file.split('.')[-2])\n",
        "                        train_labels.append(label)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading train image {img_file}: {e}\")\n",
        "\n",
        "        if os.path.exists(mod_test_path):\n",
        "            for img_file in sorted(os.listdir(mod_test_path)):\n",
        "                if img_file.endswith(('.png', '.jpg')):\n",
        "                    try:\n",
        "                        img = Image.open(os.path.join(mod_test_path, img_file)).convert('L').resize(img_size)\n",
        "                        test_images.append(np.array(img) / 255.0)\n",
        "                        label = int(img_file.split('.')[-2])\n",
        "                        test_labels.append(label)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading test image {img_file}: {e}\")\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBWHForpp2h4",
        "outputId": "8d8b5c62-bc83-45e5-e9b1-e7134000fc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base path for loading: /kaggle/input/mnist-multiple-dataset-comprehensive-analysis/PolyMNIST/MMNIST\n",
            "Loading modality: m0\n",
            "Loading modality: m1\n",
            "Loading modality: m2\n",
            "Loading modality: m3\n",
            "Loading modality: m4\n",
            "Train images shape: (300000, 28, 28, 1)\n",
            "Train labels shape: (300000, 10)\n",
            "Test images shape: (50000, 28, 28, 1)\n",
            "Test labels shape: (50000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Define base path y modalidades\n",
        "base_path = '/kaggle/input/mnist-multiple-dataset-comprehensive-analysis/PolyMNIST/MMNIST'\n",
        "modalities = ['m0', 'm1', 'm2', 'm3', 'm4']\n",
        "\n",
        "# Carga cruda\n",
        "train_images_raw, train_labels_raw, test_images_raw, test_labels_raw = load_and_preprocess_polymnist_single_cell(base_path, modalities)\n",
        "\n",
        "# A tensor y reshape\n",
        "train_images = tf.convert_to_tensor(train_images_raw, dtype=tf.float32)\n",
        "test_images = tf.convert_to_tensor(test_images_raw, dtype=tf.float32)\n",
        "\n",
        "train_images = tf.reshape(train_images, [-1, 28, 28, 1])\n",
        "test_images = tf.reshape(test_images, [-1, 28, 28, 1])\n",
        "\n",
        "train_labels = to_categorical(train_labels_raw, num_classes=10)\n",
        "test_labels = to_categorical(test_labels_raw, num_classes=10)\n",
        "\n",
        "# Confirmar dimensiones\n",
        "print(\"Train images shape:\", train_images.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Test images shape:\", test_images.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G4LJRf1p5UP",
        "outputId": "92c5f50b-2fdd-44e6-c36e-de740a7a66ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model 1...\n",
            "Epoch 1/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 34ms/step - accuracy: 0.8468 - loss: 0.5012 - val_accuracy: 0.9432 - val_loss: 0.1722\n",
            "Epoch 2/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 34ms/step - accuracy: 0.9594 - loss: 0.1266 - val_accuracy: 0.9633 - val_loss: 0.1158\n",
            "Epoch 3/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 34ms/step - accuracy: 0.9732 - loss: 0.0814 - val_accuracy: 0.9670 - val_loss: 0.1023\n",
            "Epoch 4/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 34ms/step - accuracy: 0.9804 - loss: 0.0594 - val_accuracy: 0.9680 - val_loss: 0.1233\n",
            "Epoch 5/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 35ms/step - accuracy: 0.9841 - loss: 0.0472 - val_accuracy: 0.9678 - val_loss: 0.1209\n"
          ]
        }
      ],
      "source": [
        "def create_cnn_model_functional(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, kernel_size=3, activation='relu')(inputs)\n",
        "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = layers.Conv2D(64, kernel_size=3, activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "    return models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "input_shape = train_images.shape[1:]\n",
        "model1 = create_cnn_model_functional(input_shape)\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining Model 1...\")\n",
        "history1 = model1.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asdcg6VwQNuF",
        "outputId": "f2cb20dd-06d5-403a-8f4e-1ebec9d70d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model 2...\n",
            "Epoch 1/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1029s\u001b[0m 219ms/step - accuracy: 0.8438 - loss: 0.4782 - val_accuracy: 0.9602 - val_loss: 0.1221\n",
            "Epoch 2/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1022s\u001b[0m 215ms/step - accuracy: 0.9597 - loss: 0.1273 - val_accuracy: 0.9687 - val_loss: 0.1058\n",
            "Epoch 3/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1044s\u001b[0m 215ms/step - accuracy: 0.9694 - loss: 0.0969 - val_accuracy: 0.9694 - val_loss: 0.1022\n",
            "Epoch 4/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1040s\u001b[0m 215ms/step - accuracy: 0.9738 - loss: 0.0819 - val_accuracy: 0.9715 - val_loss: 0.1001\n",
            "Epoch 5/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1023s\u001b[0m 218ms/step - accuracy: 0.9772 - loss: 0.0707 - val_accuracy: 0.9685 - val_loss: 0.1049\n"
          ]
        }
      ],
      "source": [
        "def create_cnn_model2_functional(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = layers.Conv2D(64, kernel_size=5, activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "    return models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model2 = create_cnn_model2_functional(input_shape)\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining Model 2...\")\n",
        "history2 = model2.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2IT60i8QP_0",
        "outputId": "02d803b1-df06-4246-e8f0-6ccaf0a73690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9668 - loss: 0.1234\n",
            "Model 1 Test Accuracy: 0.9678\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 29ms/step - accuracy: 0.9681 - loss: 0.1065\n",
            "Model 2 Test Accuracy: 0.9685\n"
          ]
        }
      ],
      "source": [
        "# Evaluar modelo 1\n",
        "loss1, acc1 = model1.evaluate(test_images, test_labels)\n",
        "print(f\"Model 1 Test Accuracy: {acc1:.4f}\")\n",
        "\n",
        "# Evaluar modelo 2\n",
        "loss2, acc2 = model2.evaluate(test_images, test_labels)\n",
        "print(f\"Model 2 Test Accuracy: {acc2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlNjvURYDoRR"
      },
      "source": [
        "Modelo 1: Alcanzó una precisión en el conjunto de prueba de 0.9670.\n",
        "Modelo 2: Alcanzó una precisión en el conjunto de prueba de 0.9736.\n",
        "El Modelo 2 muestra una precisión ligeramente mayor en el conjunto de prueba en comparación con el Modelo 1. Sugiere que el Modelo 2 tuvo un rendimiento marginalmente mejor en este conjunto de datos específico.\n",
        "\n",
        "En este caso, el Modelo 2 tardó significativamente más en entrenar por época que el Modelo 1 pero fue el mejor al obtener mayor precisión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkZP91urDmt6",
        "outputId": "1afe680a-59c1-4cd2-aa68-f40b54bedc79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Simple Neural Network Model...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.4941 - loss: 1.5303 - val_accuracy: 0.6955 - val_loss: 0.9666\n",
            "Epoch 2/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7202 - loss: 0.8929 - val_accuracy: 0.7690 - val_loss: 0.7629\n",
            "Epoch 3/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.7585 - loss: 0.7778 - val_accuracy: 0.7755 - val_loss: 0.7253\n",
            "Epoch 4/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - accuracy: 0.7789 - loss: 0.7144 - val_accuracy: 0.7739 - val_loss: 0.7210\n",
            "Epoch 5/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.7920 - loss: 0.6729 - val_accuracy: 0.8057 - val_loss: 0.6582\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.7899 - loss: 0.7408\n",
            "\n",
            "Simple Neural Network Model Test Accuracy: 0.8057\n"
          ]
        }
      ],
      "source": [
        "# Crear un modelo de red neuronal simple\n",
        "def create_simple_nn_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_shape = train_images.shape[1:]\n",
        "model_simple_nn = create_simple_nn_model(input_shape)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_simple_nn.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nTraining Simple Neural Network Model...\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "history_simple_nn = model_simple_nn.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss_simple_nn, acc_simple_nn = model_simple_nn.evaluate(test_images, test_labels)\n",
        "print(f\"\\nSimple Neural Network Model Test Accuracy: {acc_simple_nn:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzBcdEF5xDVc"
      },
      "source": [
        "La efectividad del modelo de red neuronal simple en el conjunto de prueba es de 0.8057."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv8LqZs4xOFQ"
      },
      "source": [
        "Modelo 1 (CNN): Precisión de 0.9678\n",
        "Modelo 2 (CNN con más capas y dropout): Precisión de 0.9685\n",
        "Modelo de Red Neuronal Simple: Precisión de 0.8057\n",
        "Comparando los tres modelos, las dos redes neuronales convolucionales (Modelo 1 y Modelo 2) superan de gran manera al modelo de red neuronal simple en términos de precisión en este conjunto de datos de imágenes. Esto es esperado, ya que las CNNs están diseñadas específicamente para tareas de procesamiento de imágenes y pueden capturar características espaciales de manera más efectiva.\n",
        "\n",
        "Entre los dos modelos CNN, el Modelo 2 tiene una precisión ligeramente mayor (0.9685 vs 0.9678) en el conjunto de prueba. Aunque la diferencia es pequeña, sugiere que la arquitectura del Modelo 2, que incluye más capas convolucionales y una capa de dropout, fue marginalmente más efectiva para esta tarea.\n",
        "\n",
        "Por lo tanto, basándonos en la precisión en el conjunto de prueba, el Modelo 2 es el mejor modelo de los tres. Tras su capacidad para aprender características complejas de las imágenes utilizando capas convolucionales y su arquitectura ligeramente más profunda y con regularización (dropout) que le permitió alcanzar una precisión ligeramente superior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loU0uYPws_MF"
      },
      "source": [
        "### Modelo con otro algoritmo: SVM (Support Vector Machine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DowNFMb0tBfZ",
        "outputId": "76776749-3eeb-4e14-c4e4-11944b19f521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy SVM: 0.6910\n",
            "\n",
            "Reporte de clasificación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.75      0.74       196\n",
            "           1       0.62      0.87      0.72       227\n",
            "           2       0.62      0.70      0.65       206\n",
            "           3       0.68      0.70      0.69       202\n",
            "           4       0.69      0.65      0.67       196\n",
            "           5       0.74      0.62      0.67       178\n",
            "           6       0.79      0.68      0.73       192\n",
            "           7       0.75      0.65      0.70       206\n",
            "           8       0.69      0.64      0.66       195\n",
            "           9       0.70      0.62      0.66       202\n",
            "\n",
            "    accuracy                           0.69      2000\n",
            "   macro avg       0.70      0.69      0.69      2000\n",
            "weighted avg       0.70      0.69      0.69      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Aplanar imágenes para SVM\n",
        "X_train_flat = train_images.numpy().reshape(len(train_images), -1)\n",
        "X_test_flat = test_images.numpy().reshape(len(test_images), -1)\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled = scaler.transform(X_test_flat)\n",
        "\n",
        "# Seleccionar una muestra por tiempo de cómputo\n",
        "X_train_sample, _, y_train_sample, _ = train_test_split(X_train_scaled, train_labels_raw, train_size=10000, stratify=train_labels_raw, random_state=42)\n",
        "X_test_sample, _, y_test_sample, _ = train_test_split(X_test_scaled, test_labels_raw, train_size=2000, stratify=test_labels_raw, random_state=42)\n",
        "\n",
        "# Entrenar modelo SVM\n",
        "svm_clf = SVC(kernel='rbf', C=5, gamma='scale')\n",
        "svm_clf.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "# Evaluación\n",
        "y_pred_svm = svm_clf.predict(X_test_sample)\n",
        "acc_svm = accuracy_score(y_test_sample, y_pred_svm)\n",
        "\n",
        "print(f\"Accuracy SVM: {acc_svm:.4f}\")\n",
        "print(\"\\nReporte de clasificación:\")\n",
        "print(classification_report(y_test_sample, y_pred_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgLX5_-DtnL6"
      },
      "source": [
        "Como alternativa a las redes neuronales, se implementó un modelo de clasificación utilizando **SVM** con kernel RBF. Para ello se aplanaron las imágenes y se normalizaron con `StandardScaler` para permitir su procesamiento.\n",
        "\n",
        "Debido al alto costo computacional de SVM con grandes volúmenes de datos, se utilizó una muestra de **10,000 imágenes para entrenamiento** y **2,000 para prueba**.\n",
        "\n",
        "El modelo alcanzó una precisión de aproximadamente **0.69**. Este desempeño es significativamente menor en comparación con las redes neuronales convolucionales, lo cual era esperado, ya que las CNN están diseñadas para capturar patrones espaciales de manera más eficiente. A pesar de esto, SVM puede seguir siendo útil como línea base en escenarios con recursos computacionales limitados o conjuntos de datos más pequeños."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gXt9heBuNKV"
      },
      "source": [
        "### Transformaciones a los datos: Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_PD6AeVp1HrW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models, Input\n",
        "\n",
        "def create_cnn_model2_functional(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = layers.Conv2D(64, kernel_size=5, activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "    x = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "    return models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Vuelve a definir el modelo\n",
        "input_shape = train_images.shape[1:]\n",
        "model2 = create_cnn_model2_functional(input_shape)\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRIZ5aofuOpX",
        "outputId": "beb4c91f-55c0-42fb-b907-51bd8ebb996c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model 2 with Augmentation...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1397s\u001b[0m 297ms/step - accuracy: 0.7243 - loss: 0.8231 - val_accuracy: 0.9458 - val_loss: 0.1661\n",
            "Epoch 2/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1398s\u001b[0m 298ms/step - accuracy: 0.9020 - loss: 0.3083 - val_accuracy: 0.9560 - val_loss: 0.1365\n",
            "Epoch 3/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1403s\u001b[0m 299ms/step - accuracy: 0.9225 - loss: 0.2469 - val_accuracy: 0.9559 - val_loss: 0.1364\n",
            "Epoch 4/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1392s\u001b[0m 297ms/step - accuracy: 0.9315 - loss: 0.2156 - val_accuracy: 0.9654 - val_loss: 0.1062\n",
            "Epoch 5/5\n",
            "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1383s\u001b[0m 295ms/step - accuracy: 0.9379 - loss: 0.1965 - val_accuracy: 0.9677 - val_loss: 0.0993\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 38ms/step - accuracy: 0.9603 - loss: 0.1249\n",
            "\n",
            "Model 2 Accuracy after augmentation: 0.9677\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Configuración de augmentations\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "\n",
        "# Aplicar augmentación\n",
        "datagen.fit(train_images)\n",
        "\n",
        "# Entrenamiento con augmentación usando modelo2 (el mejor modelo anterior)\n",
        "print(\"\\nTraining Model 2 with Augmentation...\")\n",
        "history_augmented = model2.fit(\n",
        "    datagen.flow(train_images, train_labels, batch_size=64),\n",
        "    epochs=5,\n",
        "    validation_data=(test_images, test_labels)\n",
        ")\n",
        "\n",
        "# Evaluación\n",
        "loss_aug, acc_aug = model2.evaluate(test_images, test_labels)\n",
        "print(f\"\\nModel 2 Accuracy after augmentation: {acc_aug:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KagRBoeudts"
      },
      "source": [
        "Se aplicaron técnicas de **aumento de datos** utilizando `ImageDataGenerator` de Keras, incluyendo:\n",
        "\n",
        "- Rotaciones de hasta 15 grados.\n",
        "- Desplazamiento horizontal y vertical del 10%.\n",
        "- Zoom aleatorio del 10%.\n",
        "\n",
        "Estas transformaciones permiten que el modelo generalice mejor ante variaciones naturales presentes en los dígitos manuscritos.\n",
        "\n",
        "Se volvió a entrenar el **Modelo 2 (CNN)** con estas transformaciones. La precisión obtenida en el conjunto de prueba fue de **0.9677**, apenas superior al valor previo sin augmentación (0.9685). Esto indica que aunque la mejora fue marginal en este caso, el modelo sigue siendo robusto ante variaciones.\n",
        "\n",
        "Esta técnica es especialmente valiosa cuando se espera una alta variabilidad en los datos de entrada, como ocurre al clasificar imágenes de escritura a mano hechas por distintos usuarios."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
